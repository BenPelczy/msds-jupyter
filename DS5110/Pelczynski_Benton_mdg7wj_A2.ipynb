{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91d1aef3-7d3d-4b44-96a7-c0d785c89eab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ubuntu/spark-3.3.1-bin-hadoop3'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Install findspark so that the Jupyter Notebook session knows where to find PySpark dependencies.\n",
    "import findspark\n",
    "findspark.init('/home/ubuntu/spark-3.3.1-bin-hadoop3')\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e7fc8a-9815-4d74-ae21-372ec43a4739",
   "metadata": {},
   "source": [
    "## Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33e90338-d9b8-4cb6-b2a2-78720be0cf8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/spark-3.3.1-bin-hadoop3/conf/spark-env.sh: line 80: syntax error near unexpected token `newline'\n",
      "/home/ubuntu/spark-3.3.1-bin-hadoop3/conf/spark-env.sh: line 80: `export SPARK_LOCAL_IP=<172.31.3.14>'\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/03/06 07:41:34 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                "
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# The entry point into all functionality in Spark is the SparkSession class.\n",
    "spark = (SparkSession\n",
    "\t.builder\n",
    "\t.appName(\"DS5110: A2 Simple Spark Algorithm\")\n",
    "\t.master(\"spark://172.31.3.14:7077\")\n",
    "\t.config(\"spark.executor.memory\", \"1024M\")\n",
    "\t.getOrCreate())\n",
    "\n",
    "# Read the data from a file into DataFrames\n",
    "df = spark.read.csv(\"hdfs://172.31.3.14:9000/export.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c62b7526-f9bd-4eae-ba16-b0f41b4655c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Order DataFrame by the country code alphabetically (cca2) and then timestamp\n",
    "df = df.orderBy([\"cca2\", \"timestamp\"], ascending=[True, True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae804dec-4002-487b-9769-f5f3efba28ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------+----+----+--------------------+---------+--------------------+--------+---------------+--------+------+---------+-------+----+-------------+\n",
      "|battery_level|c02_level|cca2|cca3|                  cn|device_id|         device_name|humidity|             ip|latitude|   lcd|longitude|  scale|temp|    timestamp|\n",
      "+-------------+---------+----+----+--------------------+---------+--------------------+--------+---------------+--------+------+---------+-------+----+-------------+\n",
      "|            5|     1217|  AE| ARE|United Arab Emirates|      501|device-mac-501e4O...|      48|  213.42.16.154|    24.0|yellow|     54.0|Celsius|  16|1458444054343|\n",
      "|            0|      915|  AR| ARG|           Argentina|      227|meter-gauge-2273p...|      34|  200.71.230.81|   -34.6| green|   -58.38|Celsius|  15|1458444054251|\n",
      "|            1|     1189|  AR| ARG|           Argentina|      319|meter-gauge-319Y3...|      54| 200.71.236.145|   -34.6|yellow|   -58.38|Celsius|  25|1458444054287|\n",
      "|            8|     1386|  AR| ARG|           Argentina|      763|meter-gauge-763JW...|      82|    200.55.0.70|   -34.6|yellow|   -58.38|Celsius|  21|1458444054404|\n",
      "|            0|      861|  AR| ARG|           Argentina|      943|meter-gauge-943BT...|      77|  200.59.128.19|   -34.6| green|   -58.38|Celsius|  33|1458444054435|\n",
      "|            5|      939|  AT| AUT|             Austria|       21|  device-mac-21sjz5h|      44|193.200.142.254|    48.2| green|    16.37|Celsius|  30|1458444054131|\n",
      "|            6|     1328|  AT| AUT|             Austria|       75|device-mac-75OLmC...|      96| 143.161.246.65|    48.2|yellow|    16.37|Celsius|  12|1458444054168|\n",
      "|            8|     1287|  AT| AUT|             Austria|      236|sensor-pad-2369xz...|      47|  217.25.119.17|    48.2|yellow|    16.37|Celsius|  22|1458444054256|\n",
      "|            2|     1522|  AT| AUT|             Austria|      257|meter-gauge-257AT...|      26|   87.243.133.1|    47.2|   red|    14.83|Celsius|  16|1458444054266|\n",
      "|            1|      811|  AT| AUT|             Austria|      271|meter-gauge-271BjIL0|      31|  149.148.140.1|    48.2| green|    16.37|Celsius|  16|1458444054271|\n",
      "|            7|      904|  AT| AUT|             Austria|      294|sensor-pad-294FMZ...|      26|     83.65.45.1|    48.2| green|    16.37|Celsius|  14|1458444054279|\n",
      "|            6|      917|  AT| AUT|             Austria|      369|device-mac-369rYH...|      25|  193.239.188.1|    48.2| green|    16.37|Celsius|  23|1458444054303|\n",
      "|            3|      826|  AT| AUT|             Austria|      483|device-mac-483Tyi...|      90| 84.116.245.201|    48.2| green|    16.37|Celsius|  16|1458444054339|\n",
      "|            2|      816|  AT| AUT|             Austria|      504|sensor-pad-504Kdi...|      78| 87.243.151.193|   47.27| green|     11.4|Celsius|  32|1458444054344|\n",
      "|            1|     1196|  AT| AUT|             Austria|      585|device-mac-5851AntHC|      51|   84.116.252.9|    48.2|yellow|    16.37|Celsius|  27|1458444054364|\n",
      "|            4|     1042|  AT| AUT|             Austria|      758| sensor-pad-7589QBtr|      48|   62.218.4.130|    48.2|yellow|    16.37|Celsius|  30|1458444054403|\n",
      "|            5|     1543|  AT| AUT|             Austria|      767|meter-gauge-767rd...|      65|  195.222.121.1|    48.2|   red|    16.37|Celsius|  19|1458444054405|\n",
      "|            0|      941|  AT| AUT|             Austria|      974|sensor-pad-974x9dkX1|      53| 84.116.216.166|    48.2| green|    16.37|Celsius|  26|1458444054439|\n",
      "|            8|      895|  AT| AUT|             Austria|      977|meter-gauge-977yB...|      52|     83.65.95.1|    48.2| green|    16.37|Celsius|  11|1458444054440|\n",
      "|            0|      899|  AU| AUS|           Australia|      111|device-mac-111WYt...|      32| 203.123.94.193|   -27.0| green|    133.0|Celsius|  16|1458444054189|\n",
      "+-------------+---------+----+----+--------------------+---------+--------------------+--------+---------------+--------+------+---------+-------+----+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fcb9ba63-ede1-4a3e-90ef-4ae3cd21ea3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Output the new DF results to HDFS as an HDFS file in form of csv\n",
    "df.write.format(\"csv\").mode(\"overwrite\").save(\"hdfs://172.31.3.14:9000/sorted_export.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a48a4acd-992e-45b0-9560-ee5c32da8f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f50920-0339-49e9-b0ea-2bbeb700f8e9",
   "metadata": {},
   "source": [
    "## Part 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39454694-a335-4f45-a3b1-ca405d58290e",
   "metadata": {},
   "source": [
    "### Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8a9449ff-fb20-4a9e-90d2-c81e64292639",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import sys\n",
    "from operator import add\n",
    "from typing import Iterable, Tuple\n",
    "from pyspark.resultiterable import ResultIterable\n",
    "\n",
    "\"\"\"Helper function to calculate URL contributions to the rank of other URLs\"\"\"\n",
    "def calculateRankContrib(urls: ResultIterable[str], rank: float) -> Iterable[Tuple[str, float]]:\n",
    "    num_urls = len(urls)\n",
    "    for url in urls:\n",
    "        yield (url, rank / num_urls)\n",
    "\n",
    "\n",
    "\"\"\"Helper function to parse a URLs string into URLs pair\"\"\"\n",
    "def parseNeighborURLs(urls: str) -> Tuple[str, str]:\n",
    "    parts = re.split(r'\\s+', urls)\n",
    "    return parts[0], parts[1]\n",
    "\n",
    "spark = (SparkSession\n",
    "\t.builder\n",
    "\t.appName(\"DS5110: A2 PageRank\")\n",
    "\t.master(\"spark://172.31.3.14:7077\")\n",
    "\t.config(\"spark.executor.memory\", \"2048M\")\n",
    "\t.getOrCreate())\n",
    "\n",
    "# Load input file into lines RDD\n",
    "linesRDD = spark.sparkContext.textFile(\"hdfs://172.31.3.14:9000/web-BerkStan.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d36f0f3c-d6b7-46c1-9817-80f462e1fcc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform a transformation to define a links RDD by using parseNeighborURLs\n",
    "linksRDD = linesRDD.map(lambda urls: parseNeighborURLs(urls)).distinct().groupByKey().cache()\n",
    "\n",
    "# Initialize a ranks RDD with all ranks set to 1\n",
    "ranksRDD = linksRDD.map(lambda url_neighbors: (url_neighbors[0], 1.0)).persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "93d779a0-cb4d-4c4a-b27f-340ed9e28732",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "272919: 6531.324623752469\n",
      "438238: 4335.323158564438\n",
      "571448: 2383.8976074118896\n",
      "601656: 2195.3940755967283\n",
      "316792: 1855.6908757901526\n",
      "319209: 1632.8193684975693\n",
      "184094: 1532.2842374483407\n",
      "571447: 1492.9301630938794\n",
      "401873: 1436.160093346929\n",
      "66244: 1261.5783958673323\n",
      "68949: 1260.7919421349116\n",
      "284306: 1257.2475650644851\n",
      "68948: 1251.1723536459208\n",
      "96070: 1235.298540597624\n",
      "86238: 1235.298540597624\n",
      "86239: 1235.298540597624\n",
      "95551: 1235.298540597624\n",
      "68947: 1235.2985405976237\n",
      "68946: 1235.2985405976237\n",
      "77284: 1235.2985405976237\n",
      "66909: 1235.2985405976237\n",
      "95552: 1235.2985405976235\n",
      "86237: 1235.2985405976235\n",
      "768: 1225.5975665113076\n",
      "927: 1117.8383051141836\n",
      "210376: 920.6701252803678\n",
      "95527: 919.6797146521081\n",
      "100130: 916.0190658202678\n",
      "101163: 912.5380530105941\n",
      "95018: 911.183108007798\n",
      "100646: 909.7095673033007\n",
      "96045: 904.398131580974\n",
      "66879: 895.7909746044757\n",
      "210305: 893.0386730972408\n",
      "319412: 887.9352083382674\n",
      "571451: 875.7852546255617\n",
      "570985: 871.5825582573228\n",
      "544858: 869.6096568148239\n",
      "184142: 863.2307781841791\n",
      "299039: 832.314980980729\n",
      "49176: 819.8687801616526\n",
      "299040: 784.9195782082287\n",
      "319210: 764.4429282969843\n",
      "184332: 748.1100966771176\n",
      "184279: 743.4092370378004\n",
      "743: 694.5573570089947\n",
      "313077: 681.9298001499116\n",
      "331840: 665.4905257656997\n",
      "33: 660.9927237591635\n",
      "184150: 649.4401909507089\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "path hdfs://172.31.3.14:9000/pageRanks.csv already exists.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Save rank results as a Spark DF to HDFS as an HDFS csv file\u001b[39;00m\n\u001b[1;32m     16\u001b[0m ranksDf \u001b[38;5;241m=\u001b[39m ranksRDD\u001b[38;5;241m.\u001b[39mtoDF()\n\u001b[0;32m---> 17\u001b[0m \u001b[43mranksDf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhdfs://172.31.3.14:9000/pageRanks.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/spark-3.3.1-bin-hadoop3/python/pyspark/sql/readwriter.py:968\u001b[0m, in \u001b[0;36mDataFrameWriter.save\u001b[0;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m    966\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jwrite\u001b[38;5;241m.\u001b[39msave()\n\u001b[1;32m    967\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 968\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/spark-3.3.1-bin-hadoop3/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m~/spark-3.3.1-bin-hadoop3/python/pyspark/sql/utils.py:196\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    192\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: path hdfs://172.31.3.14:9000/pageRanks.csv already exists."
     ]
    }
   ],
   "source": [
    "# PageRank Algorithm\n",
    "for iteration in range(10):\n",
    "    # Create RDD of contribution values using calculateRankContrib helper function\n",
    "    contributeRDD = linksRDD.join(ranksRDD).flatMap(lambda url_rank: calculateRankContrib(url_rank[1][0], url_rank[1][1]))\n",
    "    # Update the ranks according to contribution values\n",
    "    ranksRDD = contributeRDD.reduceByKey(add).mapValues(lambda conSum: 0.15 + 0.85 * conSum).persist()\n",
    "\n",
    "# Sort ranksRDD by rank value\n",
    "sortedRDD = ranksRDD.sortBy(lambda url: url[1], ascending=False)\n",
    "\n",
    "# Print top 50 ranks\n",
    "for url, rank in sortedRDD.take(50):\n",
    "    print(f\"{url}: {rank}\")\n",
    "\n",
    "# Save rank results as a Spark DF to HDFS as an HDFS csv file\n",
    "ranksDf = ranksRDD.toDF()\n",
    "ranksDf.write.format(\"csv\").save(\"hdfs://172.31.3.14:9000/pageRanks.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bb7b7980-a8ad-41cd-ae1f-b9522d699d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5c8009-5419-4f76-bbf7-4b164de8ccd2",
   "metadata": {},
   "source": [
    "### Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "385022cd-5071-4957-82de-cc7cd59852ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 44:====================================>                     (5 + 3) / 8]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "272919: 6531.324623752422\n",
      "438238: 4335.323158564443\n",
      "571448: 2383.8976074118846\n",
      "601656: 2195.3940755967296\n",
      "316792: 1855.6908757901415\n",
      "319209: 1632.819368497568\n",
      "184094: 1532.284237448333\n",
      "571447: 1492.930163093876\n",
      "401873: 1436.1600933469272\n",
      "66244: 1261.5783958673355\n",
      "68949: 1260.7919421349154\n",
      "284306: 1257.2475650644835\n",
      "68948: 1251.1723536459237\n",
      "77284: 1235.2985405976276\n",
      "95551: 1235.2985405976276\n",
      "95552: 1235.2985405976272\n",
      "86237: 1235.2985405976272\n",
      "68946: 1235.298540597627\n",
      "86239: 1235.298540597627\n",
      "68947: 1235.2985405976267\n",
      "66909: 1235.2985405976267\n",
      "86238: 1235.2985405976267\n",
      "96070: 1235.2985405976267\n",
      "768: 1225.5975665113033\n",
      "927: 1117.8383051141807\n",
      "210376: 920.6701252803675\n",
      "95527: 919.6797146521103\n",
      "100130: 916.0190658202699\n",
      "101163: 912.5380530105961\n",
      "95018: 911.1831080078001\n",
      "100646: 909.7095673033026\n",
      "96045: 904.3981315809759\n",
      "66879: 895.7909746044775\n",
      "210305: 893.0386730972403\n",
      "319412: 887.9352083382672\n",
      "571451: 875.7852546255596\n",
      "570985: 871.5825582573198\n",
      "544858: 869.6096568148241\n",
      "184142: 863.2307781841773\n",
      "299039: 832.3149809807288\n",
      "49176: 819.8687801616496\n",
      "299040: 784.9195782082271\n",
      "319210: 764.4429282969843\n",
      "184332: 748.1100966771176\n",
      "184279: 743.4092370378003\n",
      "743: 694.5573570089949\n",
      "313077: 681.9298001499117\n",
      "331840: 665.4905257656994\n",
      "33: 660.9927237591634\n",
      "184150: 649.4401909507038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "spark = (SparkSession\n",
    "\t.builder\n",
    "\t.appName(\"DS5110: A2 PageRank\")\n",
    "\t.master(\"spark://172.31.3.14:7077\")\n",
    "\t.config(\"spark.executor.memory\", \"2048M\")\n",
    "\t.getOrCreate())\n",
    "\n",
    "# Load input file into lines RDD\n",
    "linesRDD = spark.sparkContext.textFile(\"hdfs://172.31.3.14:9000/web-BerkStan.txt\")\n",
    "# Perform a transformation to define a links RDD by using parseNeighborURLs, and partition into 8 even chunks\n",
    "linksRDD = linesRDD.map(lambda urls: parseNeighborURLs(urls)).distinct().groupByKey().partitionBy(8).cache()\n",
    "# Initialize a ranks RDD with all ranks set to 1, and partition similarly\n",
    "ranksRDD = linksRDD.map(lambda url_neighbors: (url_neighbors[0], 1.0)).partitionBy(8).persist()\n",
    "\n",
    "# PageRank Algorithm\n",
    "for iteration in range(10):\n",
    "    # Create RDD of contribution values using calculateRankContrib helper function\n",
    "    contributeRDD = linksRDD.join(ranksRDD).flatMap(lambda url_rank: calculateRankContrib(url_rank[1][0], url_rank[1][1]))\n",
    "    ranksRDD = contributeRDD.reduceByKey(add).mapValues(lambda conSum: 0.15 + 0.85 * conSum).persist()\n",
    "\n",
    "# Sort ranksRDD by rank value\n",
    "sortedRDD = ranksRDD.sortBy(lambda url: url[1], ascending=False)\n",
    "# Print top 50 ranks\n",
    "for url, rank in sortedRDD.take(50):\n",
    "    print(f\"{url}: {rank}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f8849585-27cd-443f-9035-d0d900b60fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2aede1a-452c-49a6-a77f-08c1e6b51b61",
   "metadata": {},
   "source": [
    "### Task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d135b61f-c7ef-40d2-be96-bfe4f5d1f1d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 2) / 2]"
     ]
    }
   ],
   "source": [
    "spark = (SparkSession\n",
    "\t.builder\n",
    "\t.appName(\"DS5110: A2 PageRank\")\n",
    "\t.master(\"spark://172.31.3.14:7077\")\n",
    "\t.config(\"spark.executor.memory\", \"2048M\")\n",
    "\t.getOrCreate())\n",
    "\n",
    "# Load input file into lines RDD\n",
    "linesRDD = spark.sparkContext.textFile(\"hdfs://172.31.3.14:9000/web-BerkStan.txt\")\n",
    "# Perform a transformation to define a links RDD by using parseNeighborURLs\n",
    "linksRDD = linesRDD.map(lambda urls: parseNeighborURLs(urls)).distinct().groupByKey().cache()\n",
    "# Initialize a ranks RDD with all ranks set to 1\n",
    "ranksRDD = linksRDD.map(lambda url_neighbors: (url_neighbors[0], 1.0)).persist()\n",
    "\n",
    "# PageRank Algorithm (NOTE: Kill vm2 Worker node around iteration 5)\n",
    "for iteration in range(10):\n",
    "    # Create RDD of contribution values using calculateRankContrib helper function\n",
    "    contributeRDD = linksRDD.join(ranksRDD).flatMap(lambda url_rank: calculateRankContrib(url_rank[1][0], url_rank[1][1]))\n",
    "    # Update the ranks according to contribution values\n",
    "    ranksRDD = contributeRDD.reduceByKey(add).mapValues(lambda conSum: 0.15 + 0.85 * conSum).persist()\n",
    "\n",
    "# Sort ranksRDD by rank value\n",
    "sortedRDD = ranksRDD.sortBy(lambda url: url[1], ascending=False)\n",
    "# Print top 50 ranks\n",
    "for url, rank in sortedRDD.take(50):\n",
    "    print(f\"{url}: {rank}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d92e4d-900d-4a14-925a-28a55d001c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
